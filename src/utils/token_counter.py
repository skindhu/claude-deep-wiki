"""
Token è®¡æ•°å™¨ - ä½¿ç”¨ tiktoken ç²¾ç¡®ç»Ÿè®¡ token æ•°é‡

æ”¯æŒ Claude å’Œ OpenAI æ¨¡å‹çš„ token ç»Ÿè®¡
"""

from typing import Optional
import logging

logger = logging.getLogger(__name__)

# å°è¯•å¯¼å…¥ tiktoken
try:
    import tiktoken
    TIKTOKEN_AVAILABLE = True
except ImportError:
    TIKTOKEN_AVAILABLE = False
    logger.warning("tiktoken æœªå®‰è£…ï¼Œå°†ä½¿ç”¨ç²—ç•¥ä¼°ç®—ã€‚å®‰è£…æ–¹æ³•: pip install tiktoken")


class TokenCounter:
    """Token è®¡æ•°å™¨"""

    # Claude æ¨¡å‹æ˜ å°„åˆ°å¯¹åº”çš„ encoding
    # Claude ä½¿ç”¨ä¸ GPT-4 ç›¸åŒçš„ tokenizer (cl100k_base)
    MODEL_ENCODINGS = {
        'claude-3-5-sonnet-20241022': 'cl100k_base',
        'claude-3-5-sonnet-20240620': 'cl100k_base',
        'claude-3-opus-20240229': 'cl100k_base',
        'claude-3-sonnet-20240229': 'cl100k_base',
        'claude-3-haiku-20240307': 'cl100k_base',
        'gpt-4': 'cl100k_base',
        'gpt-4-turbo': 'cl100k_base',
        'gpt-3.5-turbo': 'cl100k_base',
    }

    def __init__(self, model: str = 'claude-3-5-sonnet-20241022'):
        """
        åˆå§‹åŒ– Token è®¡æ•°å™¨

        Args:
            model: æ¨¡å‹åç§°ï¼Œé»˜è®¤ä¸º Claude 3.5 Sonnet
        """
        self.model = model
        self.encoding = None

        if TIKTOKEN_AVAILABLE:
            try:
                # è·å–å¯¹åº”çš„ encoding
                encoding_name = self.MODEL_ENCODINGS.get(model, 'cl100k_base')
                self.encoding = tiktoken.get_encoding(encoding_name)
                logger.info(f"Token è®¡æ•°å™¨åˆå§‹åŒ–æˆåŠŸï¼Œä½¿ç”¨ encoding: {encoding_name}")
            except Exception as e:
                logger.error(f"åˆå§‹åŒ– tiktoken encoding å¤±è´¥: {e}")
                self.encoding = None

    def count_tokens(self, text: str) -> int:
        """
        ç²¾ç¡®è®¡ç®—æ–‡æœ¬çš„ token æ•°é‡

        Args:
            text: è¦ç»Ÿè®¡çš„æ–‡æœ¬

        Returns:
            token æ•°é‡
        """
        if not text:
            return 0

        if self.encoding:
            try:
                tokens = self.encoding.encode(text)
                return len(tokens)
            except Exception as e:
                logger.error(f"Token è®¡æ•°å¤±è´¥ï¼Œä½¿ç”¨ä¼°ç®—: {e}")
                return self._estimate_tokens(text)
        else:
            return self._estimate_tokens(text)

    def count_tokens_with_details(self, text: str) -> dict:
        """
        è®¡ç®— token å¹¶è¿”å›è¯¦ç»†ä¿¡æ¯

        Args:
            text: è¦ç»Ÿè®¡çš„æ–‡æœ¬

        Returns:
            {
                "tokens": int,
                "characters": int,
                "method": "precise" | "estimated",
                "char_per_token": float
            }
        """
        char_count = len(text)
        token_count = self.count_tokens(text)
        method = "precise" if self.encoding else "estimated"
        char_per_token = char_count / token_count if token_count > 0 else 0

        return {
            "tokens": token_count,
            "characters": char_count,
            "method": method,
            "char_per_token": round(char_per_token, 2)
        }

    @staticmethod
    def _estimate_tokens(text: str) -> int:
        """
        ç²—ç•¥ä¼°ç®— token æ•°é‡

        è‹±æ–‡: 1 token â‰ˆ 4 å­—ç¬¦
        ä¸­æ–‡: 1 token â‰ˆ 1.5-2 å­—ç¬¦
        æ··åˆ: å–ä¸­é—´å€¼

        Args:
            text: æ–‡æœ¬

        Returns:
            ä¼°ç®—çš„ token æ•°é‡
        """
        if not text:
            return 0

        # ç»Ÿè®¡ä¸­æ–‡å­—ç¬¦æ•°
        chinese_chars = sum(1 for char in text if '\u4e00' <= char <= '\u9fff')
        total_chars = len(text)
        english_chars = total_chars - chinese_chars

        # ä¸­æ–‡æŒ‰ 1.5 å­—ç¬¦ = 1 tokenï¼Œè‹±æ–‡æŒ‰ 4 å­—ç¬¦ = 1 token
        estimated = (chinese_chars / 1.5) + (english_chars / 4)

        return int(estimated)

    def format_stats(self, text: str, prefix: str = "") -> str:
        """
        æ ¼å¼åŒ–è¾“å‡º token ç»Ÿè®¡ä¿¡æ¯

        Args:
            text: è¦ç»Ÿè®¡çš„æ–‡æœ¬
            prefix: è¾“å‡ºå‰ç¼€

        Returns:
            æ ¼å¼åŒ–çš„ç»Ÿè®¡å­—ç¬¦ä¸²
        """
        details = self.count_tokens_with_details(text)

        method_icon = "ğŸ¯" if details["method"] == "precise" else "ğŸ“"

        return (
            f"{prefix}{method_icon} Token ç»Ÿè®¡: {details['tokens']:,} tokens "
            f"({details['characters']:,} å­—ç¬¦, "
            f"{details['char_per_token']:.1f} å­—ç¬¦/token)"
        )


# å…¨å±€å•ä¾‹
_global_counter: Optional[TokenCounter] = None


def get_token_counter(model: str = 'claude-3-5-sonnet-20241022') -> TokenCounter:
    """
    è·å–å…¨å±€ Token è®¡æ•°å™¨å®ä¾‹

    Args:
        model: æ¨¡å‹åç§°

    Returns:
        TokenCounter å®ä¾‹
    """
    global _global_counter
    if _global_counter is None or _global_counter.model != model:
        _global_counter = TokenCounter(model)
    return _global_counter


def count_tokens(text: str, model: str = 'claude-3-5-sonnet-20241022') -> int:
    """
    å¿«æ·å‡½æ•°ï¼šè®¡ç®—æ–‡æœ¬çš„ token æ•°é‡

    Args:
        text: è¦ç»Ÿè®¡çš„æ–‡æœ¬
        model: æ¨¡å‹åç§°

    Returns:
        token æ•°é‡
    """
    counter = get_token_counter(model)
    return counter.count_tokens(text)


def format_token_stats(text: str, model: str = 'claude-3-5-sonnet-20241022', prefix: str = "") -> str:
    """
    å¿«æ·å‡½æ•°ï¼šæ ¼å¼åŒ–è¾“å‡º token ç»Ÿè®¡ä¿¡æ¯

    Args:
        text: è¦ç»Ÿè®¡çš„æ–‡æœ¬
        model: æ¨¡å‹åç§°
        prefix: è¾“å‡ºå‰ç¼€

    Returns:
        æ ¼å¼åŒ–çš„ç»Ÿè®¡å­—ç¬¦ä¸²
    """
    counter = get_token_counter(model)
    return counter.format_stats(text, prefix)


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # æµ‹è¯•æ–‡æœ¬
    test_texts = [
        "Hello, world! This is a test.",
        "ä½ å¥½ï¼Œä¸–ç•Œï¼è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•ã€‚",
        "æ··åˆæ–‡æœ¬ Mixed text æµ‹è¯• test",
        "def hello():\n    print('Hello, world!')\n    return True"
    ]

    counter = TokenCounter()

    print("=" * 60)
    print("Token ç»Ÿè®¡æµ‹è¯•")
    print("=" * 60)

    for text in test_texts:
        print(f"\næ–‡æœ¬: {text}")
        details = counter.count_tokens_with_details(text)
        print(f"  Tokens: {details['tokens']}")
        print(f"  å­—ç¬¦æ•°: {details['characters']}")
        print(f"  æ–¹æ³•: {details['method']}")
        print(f"  å­—ç¬¦/token: {details['char_per_token']}")
        print(f"  {counter.format_stats(text, prefix='  ')}")

    # æµ‹è¯•å¤§æ–‡æœ¬
    print("\n" + "=" * 60)
    print("å¤§æ–‡æœ¬æµ‹è¯• (200k tokens)")
    print("=" * 60)

    if TIKTOKEN_AVAILABLE:
        # ç”Ÿæˆçº¦ 200k tokens çš„æ–‡æœ¬
        large_text = "Hello world! " * 60000  # çº¦ 180k tokens
        stats = counter.count_tokens_with_details(large_text)
        print(f"Token æ•°: {stats['tokens']:,}")
        print(f"å­—ç¬¦æ•°: {stats['characters']:,}")
        print(f"æ–¹æ³•: {stats['method']}")
    else:
        print("è¯·å®‰è£… tiktoken è¿›è¡Œç²¾ç¡®æµ‹è¯•: pip install tiktoken")

