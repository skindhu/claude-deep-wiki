"""
Semantic Analyzer Agent - è¯­ä¹‰åˆ†æ Agent

ä¸“æ³¨äºä»£ç çš„æ·±åº¦è¯­ä¹‰ç†è§£å’Œä¸šåŠ¡é€»è¾‘åˆ†æã€‚

èŒè´£:
1. ç†è§£æ¨¡å—çš„ä¸šåŠ¡ä»·å€¼å’Œæ ¸å¿ƒåŠŸèƒ½
2. æ·±å…¥åˆ†æå‡½æ•°/ç±»çš„ä¸šåŠ¡é€»è¾‘
3. æå–ä¸šåŠ¡æµç¨‹å’Œäº¤äº’å…³ç³»
4. ä½¿ç”¨éªŒè¯å·¥å…·ç¡®ä¿åˆ†æå‡†ç¡®æ€§
5. ç”Ÿæˆé¢å‘ä¸šåŠ¡çš„åŠŸèƒ½æè¿°
"""

import sys
import json
import re
from pathlib import Path
from typing import Dict, Any, List, Optional

from claude_agent_sdk import ClaudeSDKClient, ClaudeAgentOptions

# æ·»åŠ  src åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from mcp_servers.code_analysis_server import create_code_analysis_mcp_server
from config import ANTHROPIC_AUTH_TOKEN, MAX_TURNS
from utils.batch_analyzer import FileAnalysisBatchManager
from utils.semantic_prompt_builder import SemanticPromptBuilder
from utils.json_extractor import JSONExtractor

logger = None  # ç®€åŒ–æ—¥å¿—


class SemanticAnalyzerAgent:
    """è¯­ä¹‰åˆ†æ Agent - ä¸“æ³¨äºä¸šåŠ¡é€»è¾‘ç†è§£"""

    def __init__(self, debug_helper):
        """
        åˆå§‹åŒ–è¯­ä¹‰åˆ†æ Agent

        Args:
            debug_helper: DebugHelper å®ä¾‹
        """
        self.debug_helper = debug_helper
        self.last_response = ""  # è®°å½•æœ€åä¸€æ¬¡å“åº”ï¼Œç”¨äºè°ƒè¯•ä¿å­˜
        self.batch_manager = None  # å»¶è¿Ÿåˆå§‹åŒ–æ‰¹å¤„ç†ç®¡ç†å™¨

        # åˆ›å»º MCP Server
        self._mcp_server = create_code_analysis_mcp_server()

        # åˆ›å»º Claude Client
        self.client = ClaudeSDKClient(
            options=ClaudeAgentOptions(
                env={"ANTHROPIC_AUTH_TOKEN": ANTHROPIC_AUTH_TOKEN},
                mcp_servers={"code-analysis": self._mcp_server},
                allowed_tools=["code-analysis/*"],
                system_prompt="ä½ æ˜¯èµ„æ·±çš„ä»£ç æ¶æ„å¸ˆï¼Œæ“…é•¿ä»æŠ€æœ¯ä»£ç ä¸­æç‚¼ä¸šåŠ¡é€»è¾‘å’Œäº§å“åŠŸèƒ½ã€‚",
                max_turns=MAX_TURNS,
                permission_mode="bypassPermissions"
            )
        )

        self._connected = False  # è¿æ¥çŠ¶æ€

    async def analyze_semantics(
        self, structure_data: Dict[str, Any], repo_path: str
    ) -> Dict[str, Any]:
        """
        å®Œæ•´è¯­ä¹‰åˆ†æï¼ˆæŒ‰æ¨¡å—è¿›è¡Œä¸‰é˜¶æ®µåˆ†æï¼‰

        Args:
            structure_data: StructureScannerAgent çš„è¾“å‡ºç»“æœ
            repo_path: ä»“åº“æ ¹ç›®å½•è·¯å¾„

        Returns:
            {
                "modules_analysis": {
                    "æ¨¡å—å": {
                        "overview": {...},
                        "detailed_analysis": {...},
                        "validated_result": {...}
                    }
                },
                "analysis_metadata": {...}
            }
        """
        # ç¡®ä¿å·²è¿æ¥
        if not self._connected:
            await self.client.connect()
            self._connected = True

        # å°è¯•åŠ è½½å®Œæ•´ç¼“å­˜
        cached_final = self.debug_helper.load_cached_data("02_semantic_analysis_final")
        if cached_final:
            return cached_final

        # æå–æ¨¡å—åˆ—è¡¨
        modules = structure_data.get('module_hierarchy', {}).get('modules', [])
        if not modules:
            raise ValueError("è¾“å…¥æ•°æ®ä¸­æ²¡æœ‰æ¨¡å—ä¿¡æ¯")

        # æŒ‰æ¨¡å—è¿›è¡Œåˆ†æ
        modules_analysis = {}

        for idx, module in enumerate(modules, 1):
            module_name = module.get('name', f'Module_{idx}')
            print(f"  [{idx}/{len(modules)}] åˆ†ææ¨¡å—: {module_name}")

            try:
                module_result = await self._analyze_single_module(
                    module, repo_path
                )
                modules_analysis[module_name] = module_result

            except Exception as e:
                print(f"    âŒ åˆ†æå¤±è´¥: {e}")
                modules_analysis[module_name] = {
                    "error": str(e),
                    "status": "failed"
                }

        # æ„å»ºæœ€ç»ˆç»“æœ
        final_result = {
            "modules_analysis": modules_analysis,
            "analysis_metadata": {
                "total_modules": len(modules),
                "analyzed_modules": len([m for m in modules_analysis.values() if m.get("status") != "failed"]),
                "failed_modules": len([m for m in modules_analysis.values() if m.get("status") == "failed"])
            }
        }

        # ä¿å­˜æœ€ç»ˆç»“æœ
        self.debug_helper.save_stage_data(
            "02_semantic_analysis_final",
            self.last_response,
            final_result
        )

        return final_result

    async def _analyze_single_module(
        self, module: Dict[str, Any], repo_path: str
    ) -> Dict[str, Any]:
        """
        åˆ†æå•ä¸ªæ¨¡å—ï¼ˆä¸‰é˜¶æ®µï¼‰

        Args:
            module: æ¨¡å—ä¿¡æ¯
            repo_path: ä»“åº“è·¯å¾„

        Returns:
            {
                "overview": {...},
                "detailed_analysis": {...},
                "validated_result": {...}
            }
        """
        module_name = module.get('name', 'Unknown')

        # é˜¶æ®µ1: æ¦‚è§ˆåˆ†æ
        print(f"    â†’ æ¦‚è§ˆåˆ†æ...")
        cached_overview = self.debug_helper.load_cached_data(f"02_semantic_overview_{module_name}")
        if cached_overview:
            overview = cached_overview
        else:
            overview = await self._overview_analysis(module, repo_path)
            self.debug_helper.save_stage_data(
                f"02_semantic_overview_{module_name}",
                self.last_response,
                overview
            )

        # é˜¶æ®µ2: ç»†èŠ‚æŒ–æ˜ï¼ˆæ‰¹é‡å¤„ç†ï¼‰
        print(f"    â†’ ç»†èŠ‚åˆ†æ...")
        cached_details = self.debug_helper.load_cached_data(f"02_semantic_details_{module_name}")
        if cached_details:
            detailed_analysis = cached_details
        else:
            detailed_analysis = await self._detailed_analysis(module, overview, repo_path)
            self.debug_helper.save_stage_data(
                f"02_semantic_details_{module_name}",
                self.last_response,
                detailed_analysis
            )

        return {
            "overview": overview,
            "detailed_analysis": detailed_analysis,
            "status": "success"
        }

    async def _overview_analysis(
        self, module: Dict[str, Any], repo_path: str
    ) -> Dict[str, Any]:
        """
        é˜¶æ®µ1: æ¦‚è§ˆåˆ†æ

        ç†è§£æ¨¡å—çš„æ•´ä½“èŒè´£ã€æ ¸å¿ƒåŠŸèƒ½ã€ä¸šåŠ¡ä»·å€¼

        Returns:
            {
                "module_name": "...",
                "business_purpose": "...",
                "core_features": [...],
                "external_interactions": [...]
            }
        """
        module_name = module.get('name', 'Unknown')
        responsibility = module.get('responsibility', '')
        layer = module.get('layer', '')
        key_files = module.get('key_files', [])

        # å‡†å¤‡å…³é”®æ–‡ä»¶ä¿¡æ¯
        key_files_info = []
        for kf in key_files[:10]:  # æœ€å¤š10ä¸ªå…³é”®æ–‡ä»¶
            key_files_info.append({
                "path": kf.get('path', ''),
                "imports": kf.get('imports', [])[:5],  # ç®€åŒ–ï¼Œåªåˆ—å‰5ä¸ª
                "exports": kf.get('exports', [])[:5]
            })

        # ä½¿ç”¨ PromptBuilder æ„å»ºæç¤ºè¯
        prompt = SemanticPromptBuilder.build_overview_prompt(
            module_name=module_name,
            responsibility=responsibility,
            layer=layer,
            repo_path=repo_path,
            key_files_info=key_files_info
        )

        # æ¯ä¸ªæ¨¡å—ä½¿ç”¨ç‹¬ç«‹sessionï¼Œä½†åŒä¸€æ¨¡å—çš„overviewå’Œdetailså…±äº«session
        # è¿™æ ·åç»­çš„è¯¦ç»†åˆ†æå¯ä»¥åŸºäºoverviewå»ºç«‹çš„ç†è§£
        session_id = f"semantic_module_{module_name}"
        await self.client.query(prompt, session_id=session_id)

        # æ¥æ”¶å“åº”
        response_text = ""
        async for message in self.client.receive_response():
            if hasattr(message, 'content'):
                for block in message.content:
                    if hasattr(block, 'text'):
                        response_text += block.text

        self.last_response = response_text

        # æå– JSON
        overview = JSONExtractor.extract(response_text)

        if not overview or not overview.get('module_name'):
            raise ValueError("é˜¶æ®µ1å¤±è´¥: æœªè¿”å›æœ‰æ•ˆçš„æ¦‚è§ˆåˆ†ææ•°æ®")

        return overview

    async def _detailed_analysis(
        self, module: Dict[str, Any], overview: Dict[str, Any], repo_path: str
    ) -> Dict[str, Any]:
        """
        é˜¶æ®µ2: ç»†èŠ‚æŒ–æ˜ï¼ˆæ‰¹é‡å¤„ç†ç‰ˆæœ¬ï¼‰

        æ·±å…¥åˆ†ææ¨¡å—çš„æ‰€æœ‰æ–‡ä»¶

        Returns:
            {
                "files_analysis": [...],
                "batch_info": {...}
            }
        """
        module_name = module.get('name', 'Unknown')

        # è·å–æ‰€æœ‰æ–‡ä»¶å’Œå…³é”®æ–‡ä»¶
        all_files = module.get('all_files', [])
        key_files = module.get('key_files', [])

        # åˆå§‹åŒ–æ‰¹å¤„ç†ç®¡ç†å™¨
        if not self.batch_manager:
            self.batch_manager = FileAnalysisBatchManager(repo_path)

        # åˆ›å»ºæˆ–æŸ¥æ‰¾æ‰¹æ¬¡ä¸“ç”¨ç›®å½•
        batch_dir = self.debug_helper.create_batch_directory(module_name)

        # å°è¯•åŠ è½½å·²æœ‰çš„æ‰¹æ¬¡ä¿¡æ¯
        batches = self.debug_helper.load_batches_info(batch_dir, module_name)
        files_to_analyze = None

        if not batches:
            # éœ€è¦é‡æ–°è®¡ç®—æ‰¹æ¬¡
            files_to_analyze = self.batch_manager.prepare_files_with_dependencies(
                all_files, key_files
            )
            batches = self.batch_manager.create_file_batches(files_to_analyze)
            self.debug_helper.save_batches_info(batch_dir, module_name, batches, files_to_analyze)

        # æ‰¹æ¬¡å¾ªç¯å¤„ç†
        batch_results = []

        for idx, batch in enumerate(batches, 1):
            print(f"       æ‰¹æ¬¡ {idx}/{len(batches)}: {len(batch['files'])} ä¸ªæ–‡ä»¶")

            # å°è¯•åŠ è½½å·²ä¿å­˜çš„æ‰¹æ¬¡ç»“æœ
            cached_batch_result = self.debug_helper.load_batch_result(batch_dir, module_name, idx)

            if cached_batch_result and cached_batch_result.get('files_analysis'):
                batch_result = cached_batch_result
            else:
                # éœ€è¦é‡æ–°åˆ†æ
                prompt = self._build_batch_prompt(
                    module, overview, batch, repo_path, idx, len(batches)
                )

                # ä½¿ç”¨ä¸overviewç›¸åŒçš„session_idï¼Œè®©AIåˆ©ç”¨å·²å»ºç«‹çš„æ¨¡å—ç†è§£
                session_id = f"semantic_module_{module_name}"
                await self.client.query(prompt, session_id=session_id)

                # æ¥æ”¶å“åº”
                response_text = ""
                async for message in self.client.receive_response():
                    if hasattr(message, 'content'):
                        for block in message.content:
                            if hasattr(block, 'text'):
                                response_text += block.text

                self.last_response = response_text

                # æå–JSON
                batch_result = JSONExtractor.extract(response_text)

                # ä¿å­˜æ‰¹æ¬¡åŸå§‹å“åº”å’Œæå–ç»“æœ
                self.debug_helper.save_batch_result(batch_dir, module_name, idx, response_text, batch_result, batch)

            # ä¿å­˜æ‰¹æ¬¡ç»“æœåˆ°åˆ—è¡¨
            if batch_result and batch_result.get('files_analysis'):
                batch_results.append({
                    'batch_id': idx,
                    'files_analysis': batch_result['files_analysis'],
                    'batch_info': batch
                })

        # æ™ºèƒ½åˆå¹¶æ‰€æœ‰æ‰¹æ¬¡ç»“æœ
        if files_to_analyze is None:
            files_to_analyze = []
            for batch in batches:
                files_to_analyze.extend(batch.get('files', []))

        merged_result = self._merge_batch_results(
            batch_results, module, overview, files_to_analyze, batches
        )

        return merged_result

    def _build_batch_prompt(
        self,
        module: Dict[str, Any],
        overview: Dict[str, Any],
        batch: Dict[str, Any],
        repo_path: str,
        batch_idx: int,
        total_batches: int
    ) -> str:
        """
        æ„å»ºæ‰¹æ¬¡åˆ†ææç¤ºè¯

        Args:
            module: æ¨¡å—ä¿¡æ¯
            overview: æ¦‚è§ˆåˆ†æç»“æœ
            batch: æ‰¹æ¬¡ä¿¡æ¯
            repo_path: ä»“åº“è·¯å¾„
            batch_idx: å½“å‰æ‰¹æ¬¡ç´¢å¼•
            total_batches: æ€»æ‰¹æ¬¡æ•°

        Returns:
            æç¤ºè¯å­—ç¬¦ä¸²
        """
        module_name = module.get('name', 'Unknown')
        business_purpose = overview.get('business_purpose', '')
        files_to_analyze = batch['files']

        # ä½¿ç”¨ PromptBuilder æ„å»ºæç¤ºè¯
        return SemanticPromptBuilder.build_batch_analysis_prompt(
            module_name=module_name,
            business_purpose=business_purpose,
            repo_path=repo_path,
            files_to_analyze=files_to_analyze,
            batch_idx=batch_idx,
            total_batches=total_batches,
            batch_cohesion=batch.get('cohesion'),
            batch_description=batch.get('description')
        )

    def _merge_batch_results(
        self,
        batch_results: List[Dict],
        module: Dict[str, Any],
        overview: Dict[str, Any],
        all_files: List[Dict],
        batches: List[Dict]
    ) -> Dict[str, Any]:
        """
        æ™ºèƒ½åˆå¹¶æ‰€æœ‰æ‰¹æ¬¡çš„åˆ†æç»“æœ

        Args:
            batch_results: æ‰€æœ‰æ‰¹æ¬¡çš„ç»“æœåˆ—è¡¨
            module: æ¨¡å—ä¿¡æ¯
            overview: æ¦‚è§ˆåˆ†æ
            all_files: æ‰€æœ‰æ–‡ä»¶ä¿¡æ¯
            batches: æ‰¹æ¬¡ä¿¡æ¯åˆ—è¡¨

        Returns:
            åˆå¹¶åçš„å®Œæ•´ç»“æœ
        """
        # 1. ç®€å•åˆå¹¶æ‰€æœ‰æ–‡ä»¶åˆ†æ
        all_files_analysis = []
        for batch_result in batch_results:
            all_files_analysis.extend(batch_result['files_analysis'])

        # 2. å»é‡ï¼ˆå¦‚æœæœ‰é‡å¤åˆ†æçš„æ–‡ä»¶ï¼‰
        seen_files = set()
        deduplicated_analysis = []
        for file_analysis in all_files_analysis:
            file_path = file_analysis.get('file_path', '')
            if file_path and file_path not in seen_files:
                seen_files.add(file_path)
                deduplicated_analysis.append(file_analysis)

        # 3. ç»Ÿè®¡ä¿¡æ¯
        total_files = len(all_files)
        analyzed_files = len(deduplicated_analysis)
        skipped_files = total_files - analyzed_files

        # 4. æå–å…³é”®ä¸šåŠ¡å®ä½“ï¼ˆè·¨æ–‡ä»¶åˆ†æï¼‰
        all_functions = []
        all_classes = []

        for file_analysis in deduplicated_analysis:
            all_functions.extend(file_analysis.get('functions', []))
            all_classes.extend(file_analysis.get('classes', []))

        # 5. æ„å»ºè·¨æ–‡ä»¶å…³ç³»å›¾
        cross_file_relationships = self._build_cross_file_relationships(
            deduplicated_analysis
        )

        # 6. æ„å»ºæœ€ç»ˆç»“æœ
        merged_result = {
            "files_analysis": deduplicated_analysis,
            "batch_info": {
                "total_batches": len(batches),
                "total_files": total_files,
                "analyzed_files": analyzed_files,
                "skipped_files": skipped_files,
                "batch_details": [
                    {
                        "batch_id": b['batch_id'],
                        "file_count": len(b['files_analysis']),
                        "cohesion": b['batch_info']['cohesion']
                    }
                    for b in batch_results
                ]
            },
            "summary": {
                "total_functions": len(all_functions),
                "total_classes": len(all_classes),
                "has_cross_file_relationships": len(cross_file_relationships) > 0
            },
            "cross_file_relationships": cross_file_relationships
        }

        return merged_result

    def _build_cross_file_relationships(
        self, files_analysis: List[Dict]
    ) -> List[Dict]:
        """
        æ„å»ºè·¨æ–‡ä»¶çš„ä¸šåŠ¡å…³ç³»

        è¯†åˆ«æ–‡ä»¶é—´çš„å¼•ç”¨ã€ç»§æ‰¿ã€ç»„åˆç­‰å…³ç³»

        Args:
            files_analysis: æ–‡ä»¶åˆ†æç»“æœåˆ—è¡¨

        Returns:
            è·¨æ–‡ä»¶å…³ç³»åˆ—è¡¨
        """
        relationships = []

        # æ„å»ºæ–‡ä»¶åˆ°ç±»/å‡½æ•°çš„æ˜ å°„
        file_entities = {}
        for file_analysis in files_analysis:
            file_path = file_analysis.get('file_path', '')
            entities = []

            # æ”¶é›†ç±»
            for cls in file_analysis.get('classes', []):
                entities.append({
                    'type': 'class',
                    'name': cls.get('name', ''),
                    'file': file_path
                })

            # æ”¶é›†å‡½æ•°
            for func in file_analysis.get('functions', []):
                entities.append({
                    'type': 'function',
                    'name': func.get('name', ''),
                    'file': file_path
                })

            file_entities[file_path] = entities

        # ç®€å•çš„å…³ç³»è¯†åˆ«ï¼ˆåŸºäºç±»çš„ business_relationshipsï¼‰
        for file_analysis in files_analysis:
            for cls in file_analysis.get('classes', []):
                for rel in cls.get('business_relationships', []):
                    relationships.append({
                        'from_file': file_analysis.get('file_path', ''),
                        'from_entity': cls.get('name', ''),
                        'to_entity': rel.get('related_class', ''),
                        'relationship_type': rel.get('relationship_type', ''),
                        'business_meaning': rel.get('business_meaning', '')
                    })

        return relationships

    async def disconnect(self):
        """æ–­å¼€è¿æ¥å¹¶æ¸…ç†èµ„æº"""
        if self._connected:
            await self.client.disconnect()
            self._connected = False


# ============================================================================
# ç‹¬ç«‹æµ‹è¯•æ¥å£
# ============================================================================

async def test_semantic_analyzer():
    """ç‹¬ç«‹æµ‹è¯•è¯­ä¹‰åˆ†æ Agent"""
    import asyncio

    # æ·»åŠ è·¯å¾„ä»¥å¯¼å…¥å…¶ä»–æ¨¡å—
    sys.path.insert(0, str(Path(__file__).parent.parent))
    from utils.debug_helper import DebugHelper
    from agents.structure_scanner_agent import StructureScannerAgent

    # æµ‹è¯•å‚æ•°
    repo_path = "/Users/huli/svn_work/xiaoyue_sdk_hippy"  # ä¿®æ”¹ä¸ºä½ çš„æµ‹è¯•ä»“åº“è·¯å¾„

    print("=" * 60)
    print("ğŸ§ª è¯­ä¹‰åˆ†æ Agent ç‹¬ç«‹æµ‹è¯•")
    print("=" * 60)

    # åˆ›å»º Debug Helper
    debug_helper = DebugHelper(enabled=True, verbose=True)

    # æ­¥éª¤1: è·å–ç»“æ„æ•°æ®
    print("\næ­¥éª¤1: è·å–ç»“æ„æ•°æ®...")
    scanner = StructureScannerAgent(debug_helper, verbose=True)

    try:
        structure_data = await scanner.scan_repository(repo_path)
        print(f"âœ… è·å–åˆ° {len(structure_data.get('module_hierarchy', {}).get('modules', []))} ä¸ªæ¨¡å—")
    except Exception as e:
        print(f"âŒ ç»“æ„æ‰«æå¤±è´¥: {e}")
        return
    finally:
        await scanner.disconnect()

    # æ­¥éª¤2: æ‰§è¡Œè¯­ä¹‰åˆ†æ
    print("\næ­¥éª¤2: æ‰§è¡Œè¯­ä¹‰åˆ†æ...")
    analyzer = SemanticAnalyzerAgent(debug_helper, verbose=True)

    try:
        semantic_result = await analyzer.analyze_semantics(structure_data, repo_path)

        # è¾“å‡ºç»“æœæ‘˜è¦
        print("\n" + "=" * 60)
        print("ğŸ“Š è¯­ä¹‰åˆ†æç»“æœæ‘˜è¦")
        print("=" * 60)

        metadata = semantic_result.get('analysis_metadata', {})
        print(f"\næ€»æ¨¡å—æ•°: {metadata.get('total_modules', 0)}")
        print(f"åˆ†ææˆåŠŸ: {metadata.get('analyzed_modules', 0)}")
        print(f"åˆ†æå¤±è´¥: {metadata.get('failed_modules', 0)}")

        modules_analysis = semantic_result.get('modules_analysis', {})
        for i, (module_name, module_result) in enumerate(list(modules_analysis.items())[:3], 1):
            print(f"\n{i}. {module_name}")
            if module_result.get('status') == 'failed':
                print(f"   çŠ¶æ€: âŒ å¤±è´¥")
                print(f"   é”™è¯¯: {module_result.get('error', 'Unknown')}")
            else:
                overview = module_result.get('overview', {})
                print(f"   ä¸šåŠ¡ä»·å€¼: {overview.get('business_purpose', 'N/A')[:60]}...")
                features = overview.get('core_features', [])
                print(f"   æ ¸å¿ƒåŠŸèƒ½æ•°: {len(features)}")

                validated = module_result.get('validated_result', {})
                report = validated.get('validation_report', {})
                print(f"   ç½®ä¿¡åº¦: {report.get('overall_confidence', 0.0):.2f}")

        if len(modules_analysis) > 3:
            print(f"\n   ... è¿˜æœ‰ {len(modules_analysis) - 3} ä¸ªæ¨¡å—")

        print("\nâœ… æµ‹è¯•å®Œæˆï¼è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ° output/debug/ ç›®å½•")

    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
    finally:
        await analyzer.disconnect()


if __name__ == "__main__":
    import asyncio
    asyncio.run(test_semantic_analyzer())

