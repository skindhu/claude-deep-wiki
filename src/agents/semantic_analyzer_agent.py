"""
Semantic Analyzer Agent - è¯­ä¹‰åˆ†æ Agent

ä¸“æ³¨äºä»£ç çš„æ·±åº¦è¯­ä¹‰ç†è§£å’Œä¸šåŠ¡é€»è¾‘åˆ†æã€‚

èŒè´£:
1. ç†è§£æ¨¡å—çš„ä¸šåŠ¡ä»·å€¼å’Œæ ¸å¿ƒåŠŸèƒ½
2. æ·±å…¥åˆ†æå‡½æ•°/ç±»çš„ä¸šåŠ¡é€»è¾‘
3. æå–ä¸šåŠ¡æµç¨‹å’Œäº¤äº’å…³ç³»
4. ä½¿ç”¨éªŒè¯å·¥å…·ç¡®ä¿åˆ†æå‡†ç¡®æ€§
5. ç”Ÿæˆé¢å‘ä¸šåŠ¡çš„åŠŸèƒ½æè¿°
"""

import sys
import json
import re
from pathlib import Path
from typing import Dict, Any, List, Optional

from claude_agent_sdk import ClaudeSDKClient, ClaudeAgentOptions

# æ·»åŠ  src åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from mcp_servers.code_analysis_server import create_code_analysis_mcp_server
from config import ANTHROPIC_AUTH_TOKEN, MAX_TURNS
from utils.semantic_prompt_builder import SemanticPromptBuilder
from utils.json_extractor import JSONExtractor
from utils.claude_query_helper import ClaudeQueryHelper

logger = None  # ç®€åŒ–æ—¥å¿—


class SemanticAnalyzerAgent:
    """è¯­ä¹‰åˆ†æ Agent - ä¸“æ³¨äºä¸šåŠ¡é€»è¾‘ç†è§£"""

    def __init__(self, debug_helper):
        """
        åˆå§‹åŒ–è¯­ä¹‰åˆ†æ Agent

        Args:
            debug_helper: DebugHelper å®ä¾‹
        """
        self.debug_helper = debug_helper
        self.last_response = ""  # è®°å½•æœ€åä¸€æ¬¡å“åº”ï¼Œç”¨äºè°ƒè¯•ä¿å­˜

        # åˆ›å»º MCP Server
        self._mcp_server = create_code_analysis_mcp_server()

        # åˆ›å»º Claude Client
        self.client = ClaudeSDKClient(
            options=ClaudeAgentOptions(
                env={"ANTHROPIC_AUTH_TOKEN": ANTHROPIC_AUTH_TOKEN},
                mcp_servers={"code-analysis": self._mcp_server},
                allowed_tools=["code-analysis/*"],
                system_prompt="ä½ æ˜¯èµ„æ·±çš„ä»£ç æ¶æ„å¸ˆï¼Œæ“…é•¿ä»æŠ€æœ¯ä»£ç ä¸­æç‚¼ä¸šåŠ¡é€»è¾‘å’Œäº§å“åŠŸèƒ½ã€‚",
                max_turns=MAX_TURNS,
                permission_mode="bypassPermissions"
            )
        )

        self._connected = False  # è¿æ¥çŠ¶æ€

    async def analyze_semantics(
        self, structure_data: Dict[str, Any], repo_path: str
    ) -> Dict[str, Any]:
        """
        å®Œæ•´è¯­ä¹‰åˆ†æï¼ˆæŒ‰æ¨¡å—è¿›è¡Œä¸‰é˜¶æ®µåˆ†æï¼‰

        Args:
            structure_data: StructureScannerAgent çš„è¾“å‡ºç»“æœ
            repo_path: ä»“åº“æ ¹ç›®å½•è·¯å¾„

        Returns:
            {
                "modules_analysis": {
                    "æ¨¡å—å": {
                        "overview": {...},
                        "detailed_analysis": {...},
                        "validated_result": {...}
                    }
                },
                "analysis_metadata": {...}
            }
        """
        # ç¡®ä¿å·²è¿æ¥
        if not self._connected:
            await self.client.connect()
            self._connected = True

        # å°è¯•åŠ è½½å®Œæ•´ç¼“å­˜
        cached_final = self.debug_helper.load_cached_data("02_semantic_analysis_final")
        if cached_final:
            return cached_final

        # æå–æ¨¡å—åˆ—è¡¨
        modules = structure_data.get('modules', [])
        if not modules:
            raise ValueError("è¾“å…¥æ•°æ®ä¸­æ²¡æœ‰æ¨¡å—ä¿¡æ¯")

        # è¿‡æ»¤æ’é™¤çš„æ¨¡å—
        from config import EXCLUDE_MODULES
        if EXCLUDE_MODULES:
            original_count = len(modules)
            modules = [m for m in modules if m.get('name') not in EXCLUDE_MODULES]
            filtered_count = original_count - len(modules)
            if filtered_count > 0:
                print(f"  âš ï¸  å·²è¿‡æ»¤ {filtered_count} ä¸ªæ¨¡å—: {', '.join(sorted(EXCLUDE_MODULES))}")

        # æŒ‰æ¨¡å—è¿›è¡Œåˆ†æ
        modules_analysis = {}

        for idx, module in enumerate(modules, 1):
            module_name = module.get('name', f'Module_{idx}')
            print(f"  [{idx}/{len(modules)}] åˆ†ææ¨¡å—: {module_name}")

            try:
                module_result = await self._analyze_single_module(
                    module, repo_path
                )
                modules_analysis[module_name] = module_result

            except Exception as e:
                print(f"    âŒ åˆ†æå¤±è´¥: {e}")
                modules_analysis[module_name] = {
                    "error": str(e),
                    "status": "failed"
                }

        # æ„å»ºæœ€ç»ˆç»“æœ
        final_result = {
            "modules_analysis": modules_analysis,
            "analysis_metadata": {
                "total_modules": len(modules),
                "analyzed_modules": len([m for m in modules_analysis.values() if m.get("status") != "failed"]),
                "failed_modules": len([m for m in modules_analysis.values() if m.get("status") == "failed"])
            }
        }

        # ä¿å­˜æœ€ç»ˆç»“æœ
        self.debug_helper.save_stage_data(
            "02_semantic_analysis_final",
            self.last_response,
            final_result
        )

        return final_result

    async def _analyze_single_module(
        self, module: Dict[str, Any], repo_path: str
    ) -> Dict[str, Any]:
        """
        å±‚æ¬¡åŒ–åˆ†æå•ä¸ªæ¨¡å—

        åˆ†æé¡ºåºï¼š
        1. ä¸»æ¨¡å—çš„ all_filesï¼ˆå¦‚æœæœ‰ï¼‰
        2. éå†æ¯ä¸ª sub_module çš„ all_files

        Args:
            module: æ¨¡å—ä¿¡æ¯
            repo_path: ä»“åº“è·¯å¾„

        Returns:
            {
                "main_module": {...},
                "sub_modules": {...},
                "status": "success"
            }
        """
        module_name = module.get('name', 'Unknown')

        # æ£€æŸ¥å®Œæ•´ç¼“å­˜
        cache_key = f"detailed/{module_name}/complete"
        cached_complete = self.debug_helper.load_cached_data(cache_key)
        if cached_complete:
            print(f"    âœ“ åŠ è½½å®Œæ•´ç¼“å­˜")
            return cached_complete

        results = {}

        # 1. åˆ†æä¸»æ¨¡å—å±‚çº§çš„æ–‡ä»¶
        main_files = module.get('all_files', [])
        if main_files:
            print(f"    â†’ åˆ†æä¸»æ¨¡å—æ–‡ä»¶: {len(main_files)} ä¸ª")

            # æ£€æŸ¥ä¸»æ¨¡å—ç¼“å­˜
            main_cache_key = f"detailed/{module_name}/main_module"
            cached_main = self.debug_helper.load_cached_data(main_cache_key)

            if cached_main:
                print(f"       âœ“ åŠ è½½ä¸»æ¨¡å—ç¼“å­˜")
                main_result = cached_main
            else:
                main_result = await self._analyze_files_batch(
                    module_name=module_name,
                    sub_name=None,
                    scope="main",
                    files=main_files,
                    description=module.get('responsibility', ''),
                    repo_path=repo_path
                )
                # ä¿å­˜ä¸»æ¨¡å—ç»“æœ
                self.debug_helper.save_stage_data(
                    main_cache_key,
                    self.last_response,
                    main_result
                )

            results['main_module'] = main_result

        # 2. åˆ†ææ¯ä¸ªå­æ¨¡å—
        sub_modules = module.get('sub_modules', [])
        results['sub_modules'] = {}

        for idx, sub_module in enumerate(sub_modules, 1):
            sub_name = sub_module.get('name', f'SubModule_{idx}')
            sub_files = sub_module.get('all_files', [])

            if sub_files:
                print(f"    â†’ åˆ†æå­æ¨¡å— [{idx}/{len(sub_modules)}]: {sub_name} ({len(sub_files)} ä¸ªæ–‡ä»¶)")

                # æ£€æŸ¥å­æ¨¡å—ç¼“å­˜
                sub_cache_key = f"detailed/{module_name}/sub_modules/{sub_name}"
                cached_sub = self.debug_helper.load_cached_data(sub_cache_key)

                if cached_sub:
                    print(f"       âœ“ åŠ è½½å­æ¨¡å—ç¼“å­˜")
                    sub_result = cached_sub
                else:
                    sub_result = await self._analyze_files_batch(
                        module_name=module_name,
                        sub_name=sub_name,
                        scope="sub_module",
                        files=sub_files,
                        description=sub_module.get('description', ''),
                        repo_path=repo_path
                    )
                    # ä¿å­˜å­æ¨¡å—ç»“æœ
                    self.debug_helper.save_stage_data(
                        sub_cache_key,
                        self.last_response,
                        sub_result
                    )

                results['sub_modules'][sub_name] = sub_result

        # ä¿å­˜å®Œæ•´ç»“æœ
        results['status'] = 'success'
        self.debug_helper.save_stage_data(
            cache_key,
            None,
            results
        )

        return results

    async def _analyze_files_batch(
        self,
        module_name: str,
        sub_name: str,
        scope: str,
        files: List[str],
        description: str,
        repo_path: str
    ) -> Dict[str, Any]:
        """
        åˆ†æ‰¹åˆ†ææ–‡ä»¶åˆ—è¡¨

        Args:
            module_name: æ¨¡å—åç§°
            sub_name: å­æ¨¡å—åç§°ï¼ˆä¸»æ¨¡å—æ—¶ä¸ºNoneï¼‰
            scope: åˆ†æèŒƒå›´ï¼ˆ"main" æˆ– "sub_module"ï¼‰
            files: æ–‡ä»¶è·¯å¾„åˆ—è¡¨
            description: æ¨¡å—/å­æ¨¡å—æè¿°
            repo_path: ä»“åº“è·¯å¾„

        Returns:
            åˆ†æç»“æœ {
                "files_analysis": [...],
                "file_count": N
            }
        """
        # åˆ›å»ºå›ºå®šå¤§å°çš„æ‰¹æ¬¡ï¼ˆ12ä¸ªæ–‡ä»¶ï¼‰
        batches = self._create_fixed_size_batches(files, batch_size=12)

        batch_results = []
        for idx, batch in enumerate(batches, 1):
            print(f"       æ‰¹æ¬¡ [{idx}/{len(batches)}]: {len(batch)} ä¸ªæ–‡ä»¶")

            # æ£€æŸ¥æ‰¹æ¬¡ç¼“å­˜
            # ä¸»æ¨¡å—ï¼šdetailed/{module_name}/batched/{module_name}_main_batch_01
            # å­æ¨¡å—ï¼šdetailed/{module_name}/batched/{full_name}_sub_module_batch_01
            full_name = f"{module_name}.{sub_name}" if sub_name else module_name
            cache_key = f"detailed/{module_name}/batched/{full_name}_{scope}_batch_{idx:02d}"
            cached = self.debug_helper.load_cached_data(cache_key)

            if cached:
                batch_results.append(cached)
                continue

            # æ„å»ºæç¤ºè¯
            prompt = SemanticPromptBuilder.build_batch_analysis_prompt(
                module_name=full_name,
                description=description,
                repo_path=repo_path,
                files=batch,
                batch_idx=idx,
                total_batches=len(batches)
            )

            # è°ƒç”¨Claudeåˆ†æ
            response, result = await ClaudeQueryHelper.query_with_json_retry(
                client=self.client,
                prompt=prompt,
                session_id=f"semantic_{full_name}",
                max_attempts=3,
                validator=lambda r: r and r.get('files_analysis')
            )

            self.last_response = response
            batch_results.append(result)
            self.debug_helper.save_stage_data(cache_key, response, result)

        # åˆå¹¶æ‰€æœ‰æ‰¹æ¬¡ç»“æœ
        return self._merge_batch_results(batch_results, files)

    def _create_fixed_size_batches(
        self,
        files: List[str],
        batch_size: int = 8
    ) -> List[List[str]]:
        """
        åˆ›å»ºå›ºå®šå¤§å°çš„æ–‡ä»¶æ‰¹æ¬¡

        Args:
            files: æ–‡ä»¶åˆ—è¡¨
            batch_size: æ¯æ‰¹æ–‡ä»¶æ•°ï¼ˆé»˜è®¤8ä¸ªï¼ŒèŒƒå›´5-10ï¼‰

        Returns:
            æ‰¹æ¬¡åˆ—è¡¨ï¼Œæ¯ä¸ªæ‰¹æ¬¡åŒ…å«æœ€å¤šbatch_sizeä¸ªæ–‡ä»¶
        """
        batches = []
        for i in range(0, len(files), batch_size):
            batch = files[i:i + batch_size]
            batches.append(batch)
        return batches

    def _merge_batch_results(
        self,
        batch_results: List[Dict],
        files: List[str]
    ) -> Dict[str, Any]:
        """
        åˆå¹¶æ‰€æœ‰æ‰¹æ¬¡çš„åˆ†æç»“æœ

        Args:
            batch_results: æ‰€æœ‰æ‰¹æ¬¡çš„ç»“æœåˆ—è¡¨
            files: æ–‡ä»¶è·¯å¾„åˆ—è¡¨

        Returns:
            åˆå¹¶åçš„ç»“æœ {
                "files_analysis": [...],
                "file_count": N
            }
        """
        # 1. åˆå¹¶æ‰€æœ‰æ–‡ä»¶åˆ†æ
        all_files_analysis = []
        for batch_result in batch_results:
            all_files_analysis.extend(batch_result.get('files_analysis', []))

        # 2. å»é‡ï¼ˆå¦‚æœæœ‰é‡å¤åˆ†æçš„æ–‡ä»¶ï¼‰
        seen_files = set()
        deduplicated_analysis = []
        for file_analysis in all_files_analysis:
            file_path = file_analysis.get('file_path', '')
            if file_path and file_path not in seen_files:
                seen_files.add(file_path)
                deduplicated_analysis.append(file_analysis)

        # 3. æ„å»ºç»“æœ
        return {
            "files_analysis": deduplicated_analysis,
            "file_count": len(files)
        }

    async def disconnect(self):
        """æ–­å¼€è¿æ¥å¹¶æ¸…ç†èµ„æº"""
        if self._connected:
            await self.client.disconnect()
            self._connected = False


# ============================================================================
# ç‹¬ç«‹æµ‹è¯•æ¥å£
# ============================================================================

async def test_semantic_analyzer(repo_path: str = None):
    """
    ç‹¬ç«‹æµ‹è¯•è¯­ä¹‰åˆ†æ Agent

    Args:
        repo_path: ä»“åº“è·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™ä»å‘½ä»¤è¡Œå‚æ•°è¯»å–
    """
    # æ·»åŠ è·¯å¾„ä»¥å¯¼å…¥å…¶ä»–æ¨¡å—
    sys.path.insert(0, str(Path(__file__).parent.parent))
    from utils.debug_helper import DebugHelper
    from agents.structure_scanner_agent import StructureScannerAgent

    # æµ‹è¯•å‚æ•°ï¼šä»å‚æ•°æˆ–å‘½ä»¤è¡Œè·å–
    if repo_path is None:
        if len(sys.argv) > 1:
            repo_path = sys.argv[1]
        else:
            print("âŒ é”™è¯¯: è¯·æä¾›ä»“åº“è·¯å¾„ä½œä¸ºå‚æ•°")
            print("   ç”¨æ³•: python semantic_analyzer_agent.py <repo_path>")
            return

    if not repo_path or not Path(repo_path).exists():
        print(f"âŒ é”™è¯¯: ä»“åº“è·¯å¾„ä¸å­˜åœ¨: {repo_path}")
        return

    print("=" * 60)
    print("ğŸ§ª è¯­ä¹‰åˆ†æ Agent ç‹¬ç«‹æµ‹è¯•")
    print("=" * 60)

    # åˆ›å»º Debug Helper
    debug_helper = DebugHelper(enabled=True)

    # æ­¥éª¤1: è·å–ç»“æ„æ•°æ®
    print("\næ­¥éª¤1: è·å–ç»“æ„æ•°æ®...")
    scanner = StructureScannerAgent(debug_helper)

    try:
        structure_data = await scanner.scan_repository(repo_path)
        print(f"âœ… è·å–åˆ° {len(structure_data.get('modules', []))} ä¸ªæ¨¡å—")
    except Exception as e:
        print(f"âŒ ç»“æ„æ‰«æå¤±è´¥: {e}")
        return
    finally:
        await scanner.disconnect()

    # æ­¥éª¤2: æ‰§è¡Œè¯­ä¹‰åˆ†æ
    print("\næ­¥éª¤2: æ‰§è¡Œè¯­ä¹‰åˆ†æ...")
    analyzer = SemanticAnalyzerAgent(debug_helper)

    try:
        semantic_result = await analyzer.analyze_semantics(structure_data, repo_path)

        # è¾“å‡ºç»“æœæ‘˜è¦
        print("\n" + "=" * 60)
        print("ğŸ“Š è¯­ä¹‰åˆ†æç»“æœæ‘˜è¦")
        print("=" * 60)

        metadata = semantic_result.get('analysis_metadata', {})
        print(f"\næ€»æ¨¡å—æ•°: {metadata.get('total_modules', 0)}")
        print(f"åˆ†ææˆåŠŸ: {metadata.get('analyzed_modules', 0)}")
        print(f"åˆ†æå¤±è´¥: {metadata.get('failed_modules', 0)}")

        modules_analysis = semantic_result.get('modules_analysis', {})
        for i, (module_name, module_result) in enumerate(list(modules_analysis.items())[:3], 1):
            print(f"\n{i}. {module_name}")
            if module_result.get('status') == 'failed':
                print(f"   çŠ¶æ€: âŒ å¤±è´¥")
                print(f"   é”™è¯¯: {module_result.get('error', 'Unknown')}")
            else:
                # æ–°æ ¼å¼ï¼šmain_module å’Œ sub_modules
                main_module = module_result.get('main_module', {})
                sub_modules = module_result.get('sub_modules', {})

                main_file_count = main_module.get('file_count', 0)
                sub_module_count = len(sub_modules)

                print(f"   ä¸»æ¨¡å—æ–‡ä»¶æ•°: {main_file_count}")
                print(f"   å­æ¨¡å—æ•°é‡: {sub_module_count}")

                # ç»Ÿè®¡æ€»æ–‡ä»¶åˆ†ææ•°
                total_analyzed = len(main_module.get('files_analysis', []))
                for sub_result in sub_modules.values():
                    total_analyzed += len(sub_result.get('files_analysis', []))

                print(f"   æ€»åˆ†ææ–‡ä»¶æ•°: {total_analyzed}")

        if len(modules_analysis) > 3:
            print(f"\n   ... è¿˜æœ‰ {len(modules_analysis) - 3} ä¸ªæ¨¡å—")

        print("\nâœ… æµ‹è¯•å®Œæˆï¼è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ° output/debug/ ç›®å½•")

    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
    finally:
        await analyzer.disconnect()


if __name__ == "__main__":
    import asyncio
    # ä»å‘½ä»¤è¡Œå‚æ•°è¯»å–ä»“åº“è·¯å¾„
    asyncio.run(test_semantic_analyzer())

