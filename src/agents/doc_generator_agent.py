"""
æ–‡æ¡£ç”Ÿæˆ Agent
è´Ÿè´£å°†è¯­ä¹‰åˆ†æç»“æœè½¬æ¢ä¸ºäº§å“éœ€æ±‚æ–‡æ¡£ï¼ˆPRDï¼‰
"""

import os
import json
import re
import sys
from pathlib import Path
from typing import Dict, List, Any, Optional

from claude_agent_sdk import ClaudeSDKClient, ClaudeAgentOptions

# æ·»åŠ  src åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from utils.prd_prompt_builder import PRDPromptBuilder
from utils.json_extractor import JSONExtractor
from utils.debug_helper import DebugHelper
from utils.claude_query_helper import ClaudeQueryHelper
from config import ANTHROPIC_AUTH_TOKEN, MAX_TURNS, OUTPUT_DIR


class DocGeneratorAgent:
    """æ–‡æ¡£ç”Ÿæˆä»£ç†ï¼Œå°†æŠ€æœ¯åˆ†æè½¬æ¢ä¸ºäº§å“æ–‡æ¡£"""

    def __init__(self, debug_helper: DebugHelper):
        """
        åˆå§‹åŒ–æ–‡æ¡£ç”Ÿæˆä»£ç†

        Args:
            debug_helper: è°ƒè¯•åŠ©æ‰‹
        """
        self.debug_helper = debug_helper
        self.prd_dir = os.path.join(OUTPUT_DIR, "prd")

        # åˆ›å»º Claude Clientï¼ˆä¸éœ€è¦ MCP å·¥å…·ï¼Œåªéœ€è¦ç”Ÿæˆæ–‡æ¡£ï¼‰
        self.client = ClaudeSDKClient(
            options=ClaudeAgentOptions(
                env={"ANTHROPIC_AUTH_TOKEN": ANTHROPIC_AUTH_TOKEN},
                mcp_servers={},  # ä¸éœ€è¦ MCP å·¥å…·
                allowed_tools=[],  # ä¸éœ€è¦å·¥å…·
                system_prompt="ä½ æ˜¯ä¸€ä½èµ„æ·±çš„äº§å“ç»ç†ï¼Œæ“…é•¿å°†æŠ€æœ¯åˆ†æè½¬æ¢ä¸ºäº§å“éœ€æ±‚æ–‡æ¡£ã€‚",
                max_turns=MAX_TURNS,
                permission_mode="bypassPermissions"
            )
        )

        self._connected = False  # è¿æ¥çŠ¶æ€

        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(self.prd_dir, exist_ok=True)

    async def generate_prd_documents(
        self,
        semantic_result: Dict[str, Any],
        repo_path: str
    ) -> Dict[str, Any]:
        """
        ç”Ÿæˆäº§å“éœ€æ±‚æ–‡æ¡£

        Args:
            semantic_result: è¯­ä¹‰åˆ†æç»“æœ
            repo_path: ä»“åº“è·¯å¾„

        Returns:
            ç”Ÿæˆç»“æœ
        """
        # ç¡®ä¿å·²è¿æ¥
        if not self._connected:
            await self.client.connect()
            self._connected = True

        modules_analysis = semantic_result.get('modules_analysis', {})

        # é˜¶æ®µ1ï¼šäº§å“åŠŸèƒ½åŸŸæ™ºèƒ½åˆ†ç»„
        print("  â†’ é˜¶æ®µ 1/3: äº§å“åŠŸèƒ½åŸŸåˆ†ç»„...")
        product_grouping = await self._load_or_create_grouping(modules_analysis)

        if not product_grouping or not product_grouping.get('domains'):
            return {
                'success': False,
                'error': 'æ— æ³•ç”Ÿæˆäº§å“åŠŸèƒ½åŸŸåˆ†ç»„'
            }

        domains = product_grouping['domains']
        print(f"     è¯†åˆ«åˆ° {len(domains)} ä¸ªäº§å“åŠŸèƒ½åŸŸ\n")

        # é˜¶æ®µ2ï¼šæŒ‰åŠŸèƒ½åŸŸç”Ÿæˆ PRD
        print("  â†’ é˜¶æ®µ 2/3: ç”Ÿæˆ PRD æ–‡æ¡£...")
        generated_count = 0
        skipped_count = 0
        failed_domains = []

        for idx, domain in enumerate(domains, 1):
            domain_name = domain['domain_name']
            print(f"     [{idx}/{len(domains)}] {domain_name}")

            try:
                result = await self._generate_domain_prd(
                    domain,
                    modules_analysis,
                    repo_path
                )

                if result['status'] == 'generated':
                    generated_count += 1
                elif result['status'] == 'skipped':
                    skipped_count += 1

            except Exception as e:
                print(f"       âŒ å¤±è´¥: {str(e)}")
                failed_domains.append(domain_name)

        # é˜¶æ®µ3ï¼šç”Ÿæˆå¯¼èˆªç´¢å¼•
        print(f"\n  â†’ é˜¶æ®µ 3/3: ç”Ÿæˆå¯¼èˆªç´¢å¼•...")
        await self._generate_index(product_grouping, repo_path)

        return {
            'success': True,
            'output_dir': self.prd_dir,
            'domains': domains,
            'generated_count': generated_count,
            'skipped_count': skipped_count,
            'failed_count': len(failed_domains)
        }

    async def _load_or_create_grouping(
        self,
        modules_analysis: Dict[str, Any]
    ) -> Optional[Dict[str, Any]]:
        """
        åŠ è½½æˆ–åˆ›å»ºäº§å“åŠŸèƒ½åŸŸåˆ†ç»„

        Args:
            modules_analysis: æ¨¡å—åˆ†æç»“æœ

        Returns:
            äº§å“åŠŸèƒ½åŸŸåˆ†ç»„ç»“æœ
        """
        # å°è¯•åŠ è½½ç¼“å­˜
        product_grouping = self.debug_helper.load_product_grouping()
        if product_grouping:
            return product_grouping

        # æ‰§è¡Œæ™ºèƒ½åˆ†ç»„
        product_grouping = await self._analyze_product_grouping(modules_analysis)

        # ä¿å­˜åˆ†ç»„ç»“æœ
        if product_grouping:
            self.debug_helper.save_product_grouping(product_grouping)

        return product_grouping

    def _extract_module_overview(self, module_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        ä»æ–°æ ¼å¼çš„æ¨¡å—æ•°æ®ä¸­æå–æ¦‚è§ˆä¿¡æ¯

        Args:
            module_data: åŒ…å«main_moduleå’Œsub_modulesçš„æ¨¡å—æ•°æ®

        Returns:
            æ¦‚è§ˆä¿¡æ¯ {
                'business_purpose': str,
                'core_features': List[str],
                'external_interactions': List[str]
            }
        """
        # èšåˆæ‰€æœ‰æ–‡ä»¶åˆ†æ
        all_files_analysis = []

        # ä»ä¸»æ¨¡å—æ”¶é›†
        main_module = module_data.get('main_module', {})
        all_files_analysis.extend(main_module.get('files_analysis', []))

        # ä»å­æ¨¡å—æ”¶é›†
        sub_modules = module_data.get('sub_modules', {})
        for sub_module_data in sub_modules.values():
            all_files_analysis.extend(sub_module_data.get('files_analysis', []))

        # æå–ä¸šåŠ¡ç›®çš„ï¼ˆå–ç¬¬ä¸€ä¸ªæœ‰æ•ˆçš„ï¼Œæˆ–èšåˆå¤šä¸ªï¼‰
        business_purposes = []
        core_features = set()
        external_interactions = set()

        for file_analysis in all_files_analysis:
            # ä¸šåŠ¡ç›®çš„
            bp = file_analysis.get('business_purpose', '').strip()
            if bp and bp not in business_purposes:
                business_purposes.append(bp)

            # æ ¸å¿ƒåŠŸèƒ½
            features = file_analysis.get('core_features', [])
            if isinstance(features, list):
                core_features.update(features)

            # å¤–éƒ¨äº¤äº’
            interactions = file_analysis.get('external_interactions', [])
            if isinstance(interactions, list):
                external_interactions.update(interactions)

        # åˆå¹¶ä¸šåŠ¡ç›®çš„ï¼ˆå–å‰3ä¸ªæœ€é‡è¦çš„ï¼‰
        business_purpose = '; '.join(business_purposes[:3]) if business_purposes else 'æœªçŸ¥ä¸šåŠ¡ç›®çš„'

        return {
            'business_purpose': business_purpose,
            'core_features': list(core_features),
            'external_interactions': list(external_interactions)
        }

    async def _analyze_product_grouping(
        self,
        modules_analysis: Dict[str, Any]
    ) -> Optional[Dict[str, Any]]:
        """
        äº§å“åŠŸèƒ½åŸŸæ™ºèƒ½åˆ†ç»„

        Args:
            modules_analysis: æ¨¡å—åˆ†æç»“æœ

        Returns:
            äº§å“åŠŸèƒ½åŸŸåˆ†ç»„ç»“æœ
        """
        # æå–æ¨¡å—æ‘˜è¦ä¿¡æ¯ï¼ˆåŒ…æ‹¬äº¤äº’å…³ç³»ï¼‰
        modules_summary = []
        for module_name, module_data in modules_analysis.items():
            # è·³è¿‡å¤±è´¥çš„æ¨¡å—
            if module_data.get('status') == 'failed':
                continue

            # ä»æ–°æ ¼å¼ä¸­æå–æ¦‚è§ˆä¿¡æ¯ï¼šèšåˆmain_moduleå’Œsub_modulesçš„æ–‡ä»¶åˆ†æ
            overview = self._extract_module_overview(module_data)

            modules_summary.append({
                'module_name': module_name,
                'business_purpose': overview.get('business_purpose', ''),
                'core_features': overview.get('core_features', []),
                'external_interactions': overview.get('external_interactions', [])  # åŒ…å«äº¤äº’å…³ç³»
            })

        if not modules_summary:
            return None

        # æ„å»ºæç¤ºè¯
        prompt = PRDPromptBuilder.build_product_grouping_prompt(modules_summary)

        # è°ƒç”¨ Claude API
        try:
            # ä½¿ç”¨ç‹¬ç«‹çš„ session_id é¿å…ä¸Šä¸‹æ–‡ç´¯ç§¯
            # ä½¿ç”¨å¸¦é‡è¯•çš„æŸ¥è¯¢ï¼ŒéªŒè¯è¿”å›çš„JSONåŒ…å«product_domainså­—æ®µä¸”æ‰€æœ‰æ¨¡å—éƒ½è¢«åˆ†é…
            def validate_complete_grouping(r):
                if not r or not r.get('product_domains'):
                    return False
                # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰æ¨¡å—éƒ½è¢«åˆ†é…
                product_domains = r.get('product_domains', [])
                assigned_modules = set()
                for domain in product_domains:
                    for module_name in domain.get('technical_modules', []):
                        assigned_modules.add(module_name)
                    # ä¹Ÿè¦æ£€æŸ¥å­åŸŸä¸­çš„æ¨¡å—
                    for sub_domain in domain.get('sub_domains', []):
                        for module_name in sub_domain.get('technical_modules', []):
                            assigned_modules.add(module_name)

                all_module_names = set(modules_analysis.keys())
                unassigned = all_module_names - assigned_modules

                if unassigned:
                    print(f"          âš ï¸  æœ‰ {len(unassigned)} ä¸ªæ¨¡å—æœªè¢«åˆ†é…: {', '.join(list(unassigned)[:5])}{'...' if len(unassigned) > 5 else ''}")
                    return False
                return True

            response_text, grouping_data = await ClaudeQueryHelper.query_with_json_retry(
                client=self.client,
                prompt=prompt,
                session_id="doc_gen_grouping",
                max_attempts=3,
                validator=validate_complete_grouping
            )

            product_domains = grouping_data.get('product_domains', [])

            # æ„å»ºæ˜ å°„å…³ç³»
            module_to_domain_mapping = {}
            for domain in product_domains:
                domain_name = domain['domain_name']
                for module_name in domain['technical_modules']:
                    module_to_domain_mapping[module_name] = domain_name

            return {
                'domains': product_domains,
                'module_to_domain_mapping': module_to_domain_mapping
            }

        except json.JSONDecodeError as e:
            print(f"  âŒ JSON è§£æå¤±è´¥: {str(e)}")
            print(f"  å“åº”å†…å®¹: {response_text[:500]}...")
            return None
        except Exception as e:
            print(f"  âŒ æ™ºèƒ½åˆ†ç»„å¤±è´¥: {str(e)}")
            return None


    async def _generate_domain_prd(
        self,
        domain_info: Dict[str, Any],
        modules_analysis: Dict[str, Any],
        repo_path: str
    ) -> Dict[str, Any]:
        """
        ç”Ÿæˆå•ä¸ªäº§å“åŠŸèƒ½åŸŸçš„PRD

        Args:
            domain_info: åŠŸèƒ½åŸŸä¿¡æ¯
            modules_analysis: æ¨¡å—åˆ†æç»“æœ
            repo_path: ä»“åº“è·¯å¾„

        Returns:
            ç”Ÿæˆç»“æœ
        """
        domain_name = domain_info['domain_name']
        technical_modules = domain_info.get('technical_modules', [])
        sub_domains = domain_info.get('sub_domains', [])

        # æ£€æŸ¥ç¼“å­˜
        prd_file = self.debug_helper.check_prd_exists(Path(self.prd_dir), domain_name)
        if prd_file:
            return {'status': 'skipped', 'file': str(prd_file)}

        # æ”¶é›†æ‰€æœ‰éœ€è¦åˆ†æçš„æŠ€æœ¯æ¨¡å—ï¼ˆåŒ…æ‹¬å­åŸŸä¸­çš„æ¨¡å—ï¼‰
        all_modules_to_analyze = list(technical_modules)
        for sub_domain in sub_domains:
            all_modules_to_analyze.extend(sub_domain.get('technical_modules', []))

        # èšåˆè¯¥åŠŸèƒ½åŸŸä¸‹æ‰€æœ‰æŠ€æœ¯æ¨¡å—çš„æ•°æ®
        aggregated_modules_data = []
        for module_name in all_modules_to_analyze:
            if module_name in modules_analysis:
                module_data = modules_analysis[module_name]

                # è·³è¿‡å¤±è´¥çš„æ¨¡å—
                if module_data.get('status') == 'failed':
                    continue

                # ä»æ–°æ ¼å¼ä¸­æå–æ•°æ®ï¼šmain_module + sub_modules
                aggregated_modules_data.append({
                    'module_name': module_name,
                    'main_module': module_data.get('main_module', {}),
                    'sub_modules': module_data.get('sub_modules', {})
                })

        if not aggregated_modules_data:
            return {'status': 'failed', 'error': 'æ²¡æœ‰æœ‰æ•ˆæ¨¡å—æ•°æ®'}

        # ä¼°ç®—tokenå¹¶å†³å®šæ˜¯å¦éœ€è¦åˆ†æ‰¹
        estimated_tokens = self._estimate_modules_tokens(aggregated_modules_data)
        max_tokens_per_batch = 150000

        # è°ƒç”¨ Claude API
        try:
            if estimated_tokens <= max_tokens_per_batch:
                prd_content = await self._generate_prd_single_batch(
                    domain_info, aggregated_modules_data, repo_path
                )
            else:
                num_batches = (estimated_tokens // max_tokens_per_batch) + 1
                prd_content = await self._generate_prd_multi_batch(
                    domain_info, aggregated_modules_data, repo_path, num_batches
                )

            # ä¿å­˜æ–‡æ¡£
            saved_file = self.debug_helper.save_prd_document(
                Path(self.prd_dir), domain_name, prd_content
            )

            if saved_file:
                return {'status': 'generated', 'file': str(saved_file)}
            else:
                return {'status': 'failed', 'error': 'ä¿å­˜æ–‡æ¡£å¤±è´¥'}

        except Exception as e:
            print(f"  âŒ ç”Ÿæˆå¤±è´¥: {str(e)}")
            return {'status': 'failed', 'error': str(e)}

    def _validate_prd_quality(self, doc_content: str) -> List[str]:
        """
        éªŒè¯PRDè´¨é‡

        Args:
            doc_content: æ–‡æ¡£å†…å®¹

        Returns:
            è´¨é‡é—®é¢˜åˆ—è¡¨
        """
        issues = []

        # æ£€æŸ¥ç¦æ­¢çš„æŠ€æœ¯æœ¯è¯­
        forbidden_terms = [
            'function', 'method', 'class', 'object', 'API', 'endpoint',
            'parameter', 'argument', 'return', 'throw', 'catch',
            'interface', 'component', 'props', 'state'
        ]

        found_terms = []
        for term in forbidden_terms:
            # ä½¿ç”¨å•è¯è¾¹ç•ŒåŒ¹é…ï¼Œé¿å…è¯¯æŠ¥
            pattern = r'\b' + term + r'\b'
            if re.search(pattern, doc_content, re.IGNORECASE):
                found_terms.append(term)

        if found_terms:
            issues.append(f"åŒ…å«æŠ€æœ¯æœ¯è¯­: {', '.join(found_terms[:5])}")

        # æ£€æŸ¥å¿…è¦ç« èŠ‚
        required_sections = ['åŠŸèƒ½è¯¦ç»†è¯´æ˜', 'ä¸šåŠ¡æµç¨‹']
        missing_sections = []
        for section in required_sections:
            if section not in doc_content:
                missing_sections.append(section)

        if missing_sections:
            issues.append(f"ç¼ºå°‘ç« èŠ‚: {', '.join(missing_sections)}")

        # æ£€æŸ¥æ–‡æ¡£é•¿åº¦ï¼ˆå¤ªçŸ­å¯èƒ½è¯´æ˜æè¿°ä¸å¤Ÿè¯¦ç»†ï¼‰
        if len(doc_content) < 500:
            issues.append("æ–‡æ¡£å†…å®¹è¿‡çŸ­ï¼Œå¯èƒ½æè¿°ä¸å¤Ÿè¯¦ç»†")

        return issues

    async def _generate_index(
        self,
        product_grouping: Dict[str, Any],
        repo_path: str
    ) -> None:
        """
        ç”Ÿæˆå¯¼èˆªç´¢å¼•

        Args:
            product_grouping: äº§å“åŠŸèƒ½åŸŸåˆ†ç»„ç»“æœ
            repo_path: ä»“åº“è·¯å¾„
        """
        domains = product_grouping.get('domains', [])

        # å‡†å¤‡åŠŸèƒ½åŸŸä¿¡æ¯
        all_domains_info = []
        for domain in domains:
            domain_name = domain['domain_name']
            safe_domain_name = re.sub(r'[^\w\-]', '_', domain_name)
            all_domains_info.append({
                'domain_name': domain_name,
                'domain_description': domain.get('domain_description', ''),
                'business_value': domain.get('business_value', ''),
                'prd_file': f"{safe_domain_name}.md"
            })

        # æ„å»ºæç¤ºè¯
        prompt = PRDPromptBuilder.build_index_prompt(
            all_domains_info,
            repo_path
        )

        # è°ƒç”¨ Claude API
        try:
            # ä½¿ç”¨ç‹¬ç«‹çš„ session_id é¿å…ä¸Šä¸‹æ–‡ç´¯ç§¯
            index_content = await ClaudeQueryHelper.query_with_text(
                client=self.client,
                prompt=prompt,
                session_id="doc_gen_index"
            )

            # ä¿å­˜ Index.md åˆ° prd ç›®å½•
            index_file = os.path.join(self.prd_dir, "Index.md")
            with open(index_file, 'w', encoding='utf-8') as f:
                f.write(index_content)

        except Exception as e:
            print(f"     âŒ å¤±è´¥: {str(e)}")

    def _estimate_modules_tokens(self, modules_data: List[Dict]) -> int:
        """
        ä¼°ç®—æ¨¡å—æ•°æ®çš„tokenæ•°é‡

        Args:
            modules_data: æ¨¡å—æ•°æ®åˆ—è¡¨

        Returns:
            ä¼°ç®—çš„tokenæ•°é‡

        ä¼°ç®—è§„åˆ™ï¼š
        - æ¯ä¸ªå­—ç¬¦çº¦ 0.3-0.4 tokensï¼ˆä¸­è‹±æ–‡æ··åˆï¼‰
        - JSONåºåˆ—åŒ–åè®¡ç®—
        """
        json_str = json.dumps(modules_data, ensure_ascii=False)
        char_count = len(json_str)
        # ä¿å®ˆä¼°è®¡ï¼Œä½¿ç”¨ 0.35 çš„è½¬æ¢æ¯”ä¾‹
        estimated_tokens = int(char_count * 0.35)
        return estimated_tokens

    async def _generate_prd_single_batch(
        self,
        domain_info: Dict[str, Any],
        modules_data: List[Dict],
        repo_path: str
    ) -> str:
        """
        å•æ‰¹æ¬¡ç”ŸæˆPRDï¼ˆåŸæœ‰é€»è¾‘ï¼‰

        Args:
            domain_info: åŠŸèƒ½åŸŸä¿¡æ¯
            modules_data: æ¨¡å—æ•°æ®åˆ—è¡¨
            repo_path: ä»“åº“è·¯å¾„

        Returns:
            ç”Ÿæˆçš„PRDå†…å®¹
        """
        prompt = PRDPromptBuilder.build_domain_prd_prompt(
            domain_info, modules_data, repo_path
        )

        # ä½¿ç”¨ç‹¬ç«‹çš„ session_id é¿å…ä¸Šä¸‹æ–‡ç´¯ç§¯ï¼ˆæ¯ä¸ªdomainä½¿ç”¨ç‹¬ç«‹ä¼šè¯ï¼‰
        domain_name = domain_info.get('domain_name', 'unknown')
        session_id = f"doc_gen_prd_{domain_name}"

        prd_content = await ClaudeQueryHelper.query_with_text(
            client=self.client,
            prompt=prompt,
            session_id=session_id
        )

        return prd_content

    async def _generate_prd_multi_batch(
        self,
        domain_info: Dict[str, Any],
        modules_data: List[Dict],
        repo_path: str,
        num_batches: int
    ) -> str:
        """
        å¤šæ‰¹æ¬¡ç”ŸæˆPRDå¹¶æ™ºèƒ½åˆå¹¶

        ç­–ç•¥ï¼š
        1. å°†æ¨¡å—å‡åŒ€åˆ†é…åˆ°å„æ‰¹æ¬¡
        2. ç¬¬ä¸€æ‰¹ï¼šç”Ÿæˆå®Œæ•´æ¡†æ¶ + ç¬¬ä¸€éƒ¨åˆ†æ¨¡å—çš„è¯¦ç»†å†…å®¹
        3. åç»­æ‰¹æ¬¡ï¼šåªç”Ÿæˆè¯¥æ‰¹æ¬¡æ¨¡å—çš„è¯¦ç»†å†…å®¹
        4. æœ€ç»ˆåˆå¹¶ï¼šå°†æ‰€æœ‰æ‰¹æ¬¡çš„å†…å®¹æ•´åˆæˆå®Œæ•´PRD

        Args:
            domain_info: åŠŸèƒ½åŸŸä¿¡æ¯
            modules_data: æ¨¡å—æ•°æ®åˆ—è¡¨
            repo_path: ä»“åº“è·¯å¾„
            num_batches: æ‰¹æ¬¡æ•°é‡

        Returns:
            åˆå¹¶åçš„å®Œæ•´PRDå†…å®¹
        """
        domain_name = domain_info['domain_name']
        modules_per_batch = len(modules_data) // num_batches + 1

        batch_contents = []

        for batch_idx in range(num_batches):
            start_idx = batch_idx * modules_per_batch
            end_idx = min(start_idx + modules_per_batch, len(modules_data))
            batch_modules = modules_data[start_idx:end_idx]

            print(f"       æ‰¹æ¬¡ {batch_idx + 1}/{num_batches}: {len(batch_modules)} ä¸ªæ¨¡å—")

            try:
                if batch_idx == 0:
                    # ç¬¬ä¸€æ‰¹ï¼šç”Ÿæˆå®Œæ•´æ¡†æ¶
                    prompt = PRDPromptBuilder.build_domain_prd_prompt_first_batch(
                        domain_info, batch_modules, len(modules_data), repo_path
                    )
                else:
                    # åç»­æ‰¹æ¬¡ï¼šåªç”Ÿæˆè¯¥æ‰¹æ¬¡çš„è¯¦ç»†å†…å®¹
                    prompt = PRDPromptBuilder.build_domain_prd_prompt_continuation(
                        domain_info, batch_modules, batch_idx + 1, num_batches, repo_path
                    )

                # åŒä¸€domainçš„æ‰€æœ‰batchä½¿ç”¨ç›¸åŒsession_idï¼Œä¿æŒä¸Šä¸‹æ–‡è¿ç»­æ€§
                # è¿™æ ·åç»­batchå¯ä»¥å‚è€ƒç¬¬ä¸€æ‰¹å»ºç«‹çš„æ¡†æ¶å’Œé£æ ¼
                session_id = f"doc_gen_prd_{domain_name}"

                batch_content = await ClaudeQueryHelper.query_with_text(
                    client=self.client,
                    prompt=prompt,
                    session_id=session_id
                )

                batch_contents.append(batch_content)

            except Exception as e:
                print(f"          âŒ å¤±è´¥: {str(e)}")
                batch_contents.append(f"\n\n#### æ‰¹æ¬¡ {batch_idx + 1} ç”Ÿæˆå¤±è´¥\nåŸå› : {str(e)}\n\n")

        # æ™ºèƒ½åˆå¹¶æ‰€æœ‰æ‰¹æ¬¡
        merged_prd = self._merge_prd_batches(batch_contents)

        return merged_prd

    def _merge_prd_batches(
        self,
        batch_contents: List[str]
    ) -> str:
        """
        åˆå¹¶å¤šä¸ªæ‰¹æ¬¡çš„PRDå†…å®¹ï¼ˆæ”¹è¿›ç‰ˆï¼šæ›´å¥å£®ã€æ›´å‡†ç¡®ï¼‰

        ç­–ç•¥ï¼š
        - ç¬¬ä¸€æ‰¹åŒ…å«å®Œæ•´æ¡†æ¶ï¼ˆç¬¬1ç« æ¦‚è¿°ã€ç¬¬3-4ç« ï¼‰
        - åç»­æ‰¹æ¬¡åŒ…å«ç¬¬2ç« çš„éƒ¨åˆ†å†…å®¹
        - åˆå¹¶æ—¶å°†åç»­æ‰¹æ¬¡çš„å†…å®¹æ’å…¥ç¬¬2ç« å’Œç¬¬3ç« ä¹‹é—´

        æ”¹è¿›ç‚¹ï¼š
        1. ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼ç²¾ç¡®åŒ¹é…ç« èŠ‚æ ‡é¢˜ï¼ˆè¡Œé¦–ï¼‰
        2. æ¸…ç†åç»­æ‰¹æ¬¡çš„é‡å¤æ ‡é¢˜å’Œå…ƒä¿¡æ¯
        3. æ ‡å‡†åŒ–ç©ºè¡Œï¼Œä¿è¯æ ¼å¼ä¸€è‡´
        4. éªŒè¯åˆå¹¶ç»“æœçš„å®Œæ•´æ€§

        Args:
            batch_contents: å„æ‰¹æ¬¡çš„å†…å®¹åˆ—è¡¨

        Returns:
            åˆå¹¶åçš„å®Œæ•´PRD
        """
        if len(batch_contents) == 1:
            return batch_contents[0]

        if not batch_contents:
            return ""

        first_batch = batch_contents[0]

        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼ç²¾ç¡®åŒ¹é…ç¬¬3ç« æ ‡é¢˜ï¼ˆå¿…é¡»åœ¨è¡Œé¦–ï¼‰
        import re

        # åŒ¹é…å„ç§å¯èƒ½çš„ç¬¬3ç« æ ‡é¢˜æ ¼å¼
        chapter3_patterns = [
            r'^##\s*3[ã€:ï¼š.]?\s*è·¨åŠŸèƒ½äº¤äº’',  # ## 3. è·¨åŠŸèƒ½äº¤äº’
            r'^##\s*ç¬¬3ç« [ã€:ï¼š.]?\s*è·¨åŠŸèƒ½äº¤äº’',  # ## ç¬¬3ç« ï¼šè·¨åŠŸèƒ½äº¤äº’
            r'^##\s*3\s*$',  # ## 3
            r'^##\s*ç¬¬3ç« \s*$',  # ## ç¬¬3ç« 
            r'^###\s*3[ã€:ï¼š.]?\s*è·¨åŠŸèƒ½äº¤äº’',  # ### 3. è·¨åŠŸèƒ½äº¤äº’
            r'^#\s*3[ã€:ï¼š.]?\s*è·¨åŠŸèƒ½äº¤äº’',  # # 3. è·¨åŠŸèƒ½äº¤äº’
        ]

        chapter3_pos = -1
        for pattern in chapter3_patterns:
            match = re.search(pattern, first_batch, re.MULTILINE)
            if match:
                chapter3_pos = match.start()
                break

        if chapter3_pos <= 0:
            # å°è¯•æ›´å®½æ¾çš„åŒ¹é…
            fallback_match = re.search(r'^##\s*[ç¬¬]?3', first_batch, re.MULTILINE)
            if fallback_match:
                chapter3_pos = fallback_match.start()

        if chapter3_pos > 0:
            # æ‰¾åˆ°äº†ç¬¬3ç« ï¼Œè¿›è¡Œæ™ºèƒ½åˆå¹¶
            part1 = first_batch[:chapter3_pos]  # ç¬¬1-2ç« 
            part2 = first_batch[chapter3_pos:]   # ç¬¬3-4ç« 

            # æ¸…ç†åç»­æ‰¹æ¬¡çš„å†…å®¹
            cleaned_continuations = []
            for i, batch_content in enumerate(batch_contents[1:], 2):
                cleaned = self._clean_continuation_content(batch_content, i)
                if cleaned.strip():
                    cleaned_continuations.append(cleaned)
                else:
                    print(f"    âš ï¸  æ‰¹æ¬¡ {i} æ¸…ç†åå†…å®¹ä¸ºç©ºï¼Œè·³è¿‡")

            # æ ‡å‡†åŒ–ç¬¬ä¸€éƒ¨åˆ†çš„å°¾éƒ¨ç©ºè¡Œ
            part1 = part1.rstrip() + "\n\n"

            # åˆå¹¶æ‰€æœ‰ç¬¬2ç« çš„å†…å®¹
            if cleaned_continuations:
                # æ¯ä¸ªæ‰¹æ¬¡ä¹‹é—´ä¿æŒ2ä¸ªç©ºè¡Œ
                chapter2_continuation = "\n\n".join(cleaned_continuations)
                merged = part1 + chapter2_continuation + "\n\n" + part2
            else:
                # æ²¡æœ‰åç»­å†…å®¹ï¼Œç›´æ¥æ‹¼æ¥
                merged = part1 + part2
        else:
            # æ²¡æ‰¾åˆ°ç« èŠ‚æ ‡è®°ï¼Œä½¿ç”¨é™çº§ç­–ç•¥
            print("    âš ï¸  æœªèƒ½è¯†åˆ«ç« èŠ‚ç»“æ„ï¼Œä½¿ç”¨é™çº§åˆå¹¶ç­–ç•¥")

            # æ¸…ç†åç»­æ‰¹æ¬¡
            all_parts = [first_batch.rstrip()]
            for i, batch_content in enumerate(batch_contents[1:], 2):
                cleaned = self._clean_continuation_content(batch_content, i)
                if cleaned.strip():
                    all_parts.append(cleaned.rstrip())

            # ä½¿ç”¨æ˜æ˜¾çš„åˆ†éš”ç¬¦
            merged = "\n\n---\n\n".join(all_parts)

        # æ ‡å‡†åŒ–æœ€ç»ˆè¾“å‡ºï¼šå»é™¤å¤šä½™ç©ºè¡Œï¼ˆè¿ç»­3ä¸ªä»¥ä¸Šç©ºè¡Œå‹ç¼©ä¸º2ä¸ªï¼‰
        merged = re.sub(r'\n{3,}', '\n\n', merged)

        return merged.strip() + "\n"

    def _clean_continuation_content(self, content: str, batch_num: int) -> str:
        """
        æ¸…ç†åç»­æ‰¹æ¬¡çš„å†…å®¹ï¼Œå»é™¤é‡å¤æ ‡é¢˜å’Œå…ƒä¿¡æ¯

        Args:
            content: æ‰¹æ¬¡å†…å®¹
            batch_num: æ‰¹æ¬¡ç¼–å·

        Returns:
            æ¸…ç†åçš„å†…å®¹
        """
        import re

        # å»é™¤å‰åç©ºç™½
        cleaned = content.strip()

        # å»é™¤å¯èƒ½çš„ç¬¬2ç« é‡å¤æ ‡é¢˜ï¼ˆå„ç§æ ¼å¼ï¼‰
        chapter2_headers = [
            r'^##\s*ç¬¬?2ç« [ã€:ï¼š.]?\s*åŠŸèƒ½è¯¦ç»†è¯´æ˜\s*$',
            r'^##\s*2[ã€:ï¼š.]?\s*åŠŸèƒ½è¯¦ç»†è¯´æ˜\s*$',
            r'^##\s*ç¬¬?2ç« \s*$',
            r'^##\s*2\s*$',
            r'^###\s*ç¬¬?2ç« ',
            r'^#\s*ç¬¬?2ç« ',
        ]

        for pattern in chapter2_headers:
            # åªåˆ é™¤å¼€å¤´çš„ç« èŠ‚æ ‡é¢˜
            cleaned = re.sub(pattern, '', cleaned, flags=re.MULTILINE, count=1)
            cleaned = cleaned.lstrip('\n')

        # å»é™¤å¯èƒ½çš„å…ƒä¿¡æ¯è¯´æ˜ï¼ˆå¦‚"ç»§ç»­ç¬¬2ç« "ã€"æœ¬æ‰¹æ¬¡ç»§ç»­æè¿°"ç­‰ï¼‰
        meta_patterns = [
            r'^.*?ç»§ç»­.*?ç¬¬[2äºŒ]ç« .*?$',
            r'^.*?æœ¬æ‰¹æ¬¡.*?$',
            r'^.*?æ¥ä¸Š.*?$',
        ]

        lines = cleaned.split('\n')
        filtered_lines = []
        for line in lines:
            is_meta = False
            for pattern in meta_patterns:
                if re.match(pattern, line.strip(), re.IGNORECASE):
                    is_meta = True
                    break
            if not is_meta:
                filtered_lines.append(line)

        cleaned = '\n'.join(filtered_lines).strip()

        return cleaned

    async def disconnect(self):
        """æ–­å¼€è¿æ¥å¹¶æ¸…ç†èµ„æº"""
        if self._connected:
            await self.client.disconnect()
            self._connected = False


# ============================================================================
# ç‹¬ç«‹æµ‹è¯•å…¥å£
# ============================================================================

async def test_doc_generator():
    """æµ‹è¯•æ–‡æ¡£ç”ŸæˆåŠŸèƒ½"""
    import sys
    from pathlib import Path

    # æ·»åŠ  src åˆ°è·¯å¾„
    sys.path.insert(0, str(Path(__file__).parent.parent))

    from utils.debug_helper import DebugHelper

    print("\n" + "="*80)
    print("ğŸ§ª æµ‹è¯•æ–‡æ¡£ç”Ÿæˆ Agent")
    print("="*80)

    # åˆå§‹åŒ–
    debug_helper = DebugHelper(enabled=True, verbose=True)
    generator = DocGeneratorAgent(debug_helper, verbose=True)

    # åŠ è½½è¯­ä¹‰åˆ†æç»“æœ
    print("\nğŸ“‚ åŠ è½½è¯­ä¹‰åˆ†æç»“æœ...")
    semantic_result = debug_helper.load_cached_data("02_semantic_analysis_final")

    if not semantic_result:
        print("âŒ æœªæ‰¾åˆ°è¯­ä¹‰åˆ†æç»“æœ")
        print("   è¯·å…ˆè¿è¡Œ: python -m src.agents.semantic_analyzer_agent")
        return

    modules_count = len(semantic_result.get('modules_analysis', {}))
    print(f"âœ… å·²åŠ è½½ {modules_count} ä¸ªæ¨¡å—çš„åˆ†æç»“æœ")

    # è·å–ä»“åº“è·¯å¾„ï¼ˆä»å½“å‰å·¥ä½œç›®å½•æ¨æ–­ï¼‰
    repo_path = str(Path.cwd())
    print(f"ğŸ“ ä»“åº“è·¯å¾„: {repo_path}")

    # ç”Ÿæˆ PRD æ–‡æ¡£
    try:
        result = await generator.generate_prd_documents(
            semantic_result,
            repo_path
        )

        if result.get('success'):
            print("\n" + "="*80)
            print("âœ… æµ‹è¯•å®Œæˆï¼")
            print("="*80)
            print(f"ğŸ“„ PRD è¾“å‡ºç›®å½•: {result['output_dir']}")
            print(f"ğŸ¯ äº§å“åŠŸèƒ½åŸŸæ•°é‡: {len(result['domains'])}")
            print(f"âœï¸  æ–°ç”Ÿæˆæ–‡æ¡£: {result['generated_count']} ä¸ª")
            print(f"ğŸ“¦ è·³è¿‡æ–‡æ¡£: {result['skipped_count']} ä¸ª")
            if result.get('failed_count', 0) > 0:
                print(f"âŒ å¤±è´¥æ–‡æ¡£: {result['failed_count']} ä¸ª")

            print("\nğŸ“‹ ç”Ÿæˆçš„åŠŸèƒ½åŸŸ:")
            for i, domain in enumerate(result['domains'], 1):
                print(f"  {i}. {domain['domain_name']}")
                print(f"     - {domain.get('domain_description', 'N/A')}")
        else:
            print(f"\nâŒ ç”Ÿæˆå¤±è´¥: {result.get('error')}")

    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
    finally:
        await generator.disconnect()


if __name__ == "__main__":
    import asyncio
    asyncio.run(test_doc_generator())

